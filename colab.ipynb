{"cells":[{"cell_type":"markdown","metadata":{"id":"Q0i46KQQomoE"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJCyYOSaomoJ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","Project_path = '/content/gdrive/MyDrive/CS229-CS230-Project'\n","\n","%cd $Project_path\n","%cd StableDiffusionProject/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boO8d2f0omoK"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Huwd7j2HomoL"},"outputs":[],"source":["from google.colab import output\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgygCROwpWIz"},"outputs":[],"source":["import os\n","os.environ['HF_HOME'] = Project_path + '/cache/huggingface'"]},{"cell_type":"markdown","metadata":{},"source":["# Auto reload"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"SjA25I_PomoL"},"source":["# Hugging face login"]},{"cell_type":"markdown","metadata":{"id":"Z-xjNr3gomoM"},"source":["If you have not create an account with hugging face and get a write token here \n","\n","Also make sure you come to here https://huggingface.co/CompVis/stable-diffusion-v1-4 and accept the condition to access the repo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wbiXi7NqomoM"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","# Tommy's token: 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\n","notebook_login()"]},{"cell_type":"markdown","metadata":{"id":"ClORjFPeomoN"},"source":["# Pipeline"]},{"cell_type":"markdown","metadata":{"id":"-UFeYacPomoN"},"source":["Load pipeline from pretrain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlISwC0zomoO"},"outputs":[],"source":["from pipeline.text_to_image_pipeline import Text2ImagePipeline\n","import torch\n","\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","pipeline = Text2ImagePipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n","pipeline.to(torch_device)\n","pass # skip output"]},{"cell_type":"markdown","metadata":{"id":"P-kgMRGxomoQ"},"source":["## Text to Image, single image "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtkFIcgMomoP"},"outputs":[],"source":["import torch\n","prompt = [\"Ragdoll cat on the sofa\"]\n","# prompt = [\"Many years ago, there was an Emperor who was so very fond of new clothes that he spent all his money on them. \"]\n","\n","height = 512                        # default height of Stable Diffusion\n","width = 512                        # default width of Stable Diffusion\n","\n","num_inference_steps = 100            # Number of denoising steps\n","\n","guidance_scale = 7.5                # Scale for classifier-free guidance\n","\n","seed = 2\n","generator = torch.Generator(device=torch_device)\n","generator = generator.manual_seed(seed)\n","\n","batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdknTuEHomoQ"},"outputs":[],"source":["out = pipeline(prompt=prompt, height=height, width=width,\n","                num_inference_steps=num_inference_steps,\n","                guidance_scale=guidance_scale,\n","                generator=generator,\n","                output_type='numpy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baIWFBkgomoQ"},"outputs":[],"source":["import matplotlib.pyplot as plt \n","\n","plt.imshow(out[0])\n","plt.axis('off')\n","plt.title(f'prompt: {prompt[0]}')\n","pass"]},{"cell_type":"markdown","metadata":{"id":"BWgvxmfWomoR"},"source":["## Text to Image, full steps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtkFIcgMomoP"},"outputs":[],"source":["import torch\n","prompt = [\"Castle on the hill, in water color style\"]\n","\n","height = 512                        # default height of Stable Diffusion\n","width = 512                         # default width of Stable Diffusion\n","\n","num_inference_steps = 100            # Number of denoising steps\n","\n","guidance_scale = 7.5                # Scale for classifier-free guidance\n","\n","seed = 32\n","generator = torch.Generator(device=torch_device)\n","generator = generator.manual_seed(seed)\n","\n","batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUl97xUtomoR"},"outputs":[],"source":["# reinitialize generator to generate the same image\n","generator = torch.Generator(device=torch_device)\n","generator = generator.manual_seed(seed)\n","\n","out = pipeline(prompt=prompt, height=height, width=width,\n","                num_inference_steps=num_inference_steps,\n","                guidance_scale=guidance_scale,\n","                generator=generator,\n","                output_type='full_steps')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyFPfiYDomoS"},"outputs":[],"source":["from common.visualization import visualize_diffusion_step_gif\n","# this might take a while\n","visualize_diffusion_step_gif(out, prompt[0], 'out.gif')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ulmWXpGomoS"},"outputs":[],"source":["from IPython.display import Image\n","\n","display(Image(data=open('out.gif','rb').read(), format='png'))"]},{"cell_type":"markdown","metadata":{"id":"RZ1grBckomoS"},"source":["## Image to Image "]},{"cell_type":"markdown","metadata":{},"source":["Initialize pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLkY_4bwomoU"},"outputs":[],"source":["from pipeline.image_to_image_pipeline import Image2ImagePipeline\n","import torch\n","\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","pipeline = Image2ImagePipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n","pipeline.to(torch_device)\n","pass # skip output"]},{"cell_type":"markdown","metadata":{"id":"DjPHqDzKomoT"},"source":["Handraw Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyonsHfZomoT"},"outputs":[],"source":["import PIL\n","import numpy as np\n","import matplotlib.pyplot as plt \n","\n","init_image = PIL.Image.open('docs/hand_draw.png')\n","init_image = np.array(init_image)\n","plt.imshow(init_image)\n","plt.axis('off')\n","plt.title(f'hand drawn')\n","pass"]},{"cell_type":"markdown","metadata":{"id":"zEz5jV3xomoT"},"source":["config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtkFIcgMomoP"},"outputs":[],"source":["import torch\n","prompt = [\"Castle on the hill, in water color style\"]\n","\n","height = 512                        # default height of Stable Diffusion\n","width = 512                         # default width of Stable Diffusion\n","\n","num_inference_steps = 100            # Number of denoising steps\n","strength = 0.8                      # how much to deviate\n","guidance_scale = 7.5                # Scale for classifier-free guidance\n","\n","seed = 32\n","generator = torch.Generator(device=torch_device)\n","generator = generator.manual_seed(seed)\n","\n","batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvZ_qkyqomoU"},"outputs":[],"source":["out = pipeline(prompt=prompt, init_image=init_image,\n","                strength = strength,\n","                height=height, width=width,\n","                num_inference_steps=num_inference_steps,\n","                guidance_scale=guidance_scale,\n","                generator=generator,\n","                output_type='numpy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ZsXHxyyomoU"},"outputs":[],"source":["plt.imshow(out[0])\n","plt.axis('off')\n","plt.title(f'prompt: {prompt[0]}')\n","pass"]},{"cell_type":"markdown","metadata":{},"source":["## Image Inpaint\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Initialize pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pipeline.inpaint_pipeline import InpaintPipeline\n","import torch\n","\n","torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","pipeline = InpaintPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n","pipeline.to(torch_device)\n","pass # skip output"]},{"cell_type":"markdown","metadata":{},"source":["Image and mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import PIL\n","import numpy as np\n","import matplotlib.pyplot as plt \n","\n","init_image = PIL.Image.open('docs/hill.jpg')\n","init_image = np.array(init_image)\n","\n","mask = np.zeros((init_image.shape[:-1]))\n","mask[100:300, 100:400] = 1\n","\n","fig = plt.figure(figsize=(10, 5))\n","ax = fig.add_subplot(121)\n","ax.imshow(init_image)\n","ax.axis('off')\n","ax.set_title(f'Initial Image')\n","ax = fig.add_subplot(122)\n","ax.imshow(init_image, )\n","ax.imshow(mask, alpha=mask, cmap='Set1')\n","ax.axis('off')\n","ax.set_title(f'Initial Image with mask')\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gtkFIcgMomoP"},"outputs":[],"source":["import torch\n","prompt = [\"Castle on the hill, in water color style\"]\n","\n","num_inference_steps = 100            # Number of denoising steps\n","\n","strength = 0.8\n","guidance_scale = 7.5               # Scale for classifier-free guidance\n","\n","seed = 32\n","generator = torch.Generator(device=torch_device)\n","generator = generator.manual_seed(seed)\n","\n","batch_size = 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["out = pipeline(prompt=prompt, \n","                init_image=init_image,\n","                mask_image=mask,\n","                strength = strength,\n","                num_inference_steps=num_inference_steps,\n","                guidance_scale=guidance_scale,\n","                generator=generator,\n","                output_type='numpy')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(out[0])\n","plt.axis('off')\n","plt.title(f'prompt: {prompt[0]}')\n","pass"]},{"cell_type":"markdown","metadata":{"id":"wqR-CzN6omoV"},"source":["# Misc\n","\n","Load in all the components"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJ7VU7WkomoV"},"outputs":[],"source":["from transformers import CLIPTextModel, CLIPTokenizer\n","from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n","\n","# 1. Load the autoencoder model which will be used to decode the latents into image space. \n","vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n","\n","# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n","tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n","text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n","\n","# 3. The UNet model for generating the latents.\n","unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"]},{"cell_type":"markdown","metadata":{"id":"B9UELeCbomoV"},"source":["load scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vX0UTRjomoV"},"outputs":[],"source":["from diffusers import LMSDiscreteScheduler, DDIMScheduler, PNDMScheduler\n","\n","scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","# scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n","# scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 (conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"34c766efb3cdeee8674807e33bf5c996af966171af4483c2af6a875a9d38b60a"}}},"nbformat":4,"nbformat_minor":0}
