{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "Project_path = '/content/gdrive/MyDrive/CS229-CS230-Project'\n",
    "\n",
    "%cd $Project_path\n",
    "%cd StableDiffusionProject/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.4.2\n",
    "!pip install transformers==4.23.1\n",
    "!pip install scipy==1.9.2\n",
    "!pip install ftfy==6.1.1\n",
    "!pip install ipywidgets==7.7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging face login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not create an account with hugging face and get a write token here \n",
    "\n",
    "Also make sure you come to here https://huggingface.co/CompVis/stable-diffusion-v1-4 and accept the condition to access the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# Tommy's token: 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\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pipeline from pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.text_to_image_pipeline import Text2ImagePipeline\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = Text2ImagePipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipeline.to(torch_device)\n",
    "pass # skip output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "prompt = [\"Castle on the hill, in water color style\"]\n",
    "\n",
    "height = 512                        # default height of Stable Diffusion\n",
    "width = 512                         # default width of Stable Diffusion\n",
    "\n",
    "num_inference_steps = 100            # Number of denoising steps\n",
    "\n",
    "guidance_scale = 7.5                # Scale for classifier-free guidance\n",
    "\n",
    "seed = 32\n",
    "generator = torch.Generator(device=torch_device)\n",
    "generator = generator.manual_seed(seed)\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Image, single image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pipeline(prompt=prompt, height=height, width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                output_type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.imshow(out[0])\n",
    "plt.axis('off')\n",
    "plt.title(f'prompt: {prompt[0]}')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Image, full steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinitialize generator to generate the same image\n",
    "generator = torch.Generator(device=torch_device)\n",
    "generator = generator.manual_seed(seed)\n",
    "\n",
    "out = pipeline(prompt=prompt, height=height, width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                output_type='full_steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.visualization import visualize_diffusion_step_gif\n",
    "\n",
    "visualize_diffusion_step_gif(out, prompt[0], 'out.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "display(Image(data=open('out.gif','rb').read(), format='png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image to Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handraw Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "init_image = PIL.Image.open('docs/hand_draw.png')\n",
    "init_image = np.array(init_image)\n",
    "\n",
    "plt.imshow(init_image)\n",
    "plt.axis('off')\n",
    "plt.title(f'hand drawn')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.image_to_image_pipeline import Image2ImagePipeline\n",
    "import torch\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline = Image2ImagePipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "pipeline.to(torch_device)\n",
    "pass # skip output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pipeline(prompt=prompt, init_image=init_image,\n",
    "                strength = 0.8,\n",
    "                height=height, width=width,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                output_type='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out[0])\n",
    "plt.axis('off')\n",
    "plt.title(f'prompt: {prompt[0]}')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc\n",
    "\n",
    "Load in all the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "# 1. Load the autoencoder model which will be used to decode the latents into image space. \n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "# 2. Load the tokenizer and text encoder to tokenize and encode the text. \n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# 3. The UNet model for generating the latents.\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LMSDiscreteScheduler, DDIMScheduler, PNDMScheduler\n",
    "\n",
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "# scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "# scheduler = PNDMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34c766efb3cdeee8674807e33bf5c996af966171af4483c2af6a875a9d38b60a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
